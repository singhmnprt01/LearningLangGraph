{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b21b7e9",
   "metadata": {},
   "source": [
    "# LangGraph Calculator Agent\n",
    "\n",
    "This notebook demonstrates how to build a **LangGraph agent** that can perform multiple calculations based on natural language user questions.\n",
    "\n",
    "## What we'll build:\n",
    "- A calculator agent with multiple mathematical tools (add, subtract, multiply, divide, power, square root, percentage)\n",
    "- The agent can handle complex multi-step calculations\n",
    "- It can understand natural language queries and break them down into calculation steps\n",
    "\n",
    "## Requirements:\n",
    "```bash\n",
    "pip install langgraph langchain-openai langchain-core openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4793b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install langgraph langchain-openai langchain-core openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719165a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "import math\n",
    "\n",
    "# LangGraph imports for building the agent graph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# LangChain imports for messages and LLM\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ef142",
   "metadata": {},
   "source": [
    "## Step 1: Define Calculation Tools\n",
    "\n",
    "We'll create individual tools for different mathematical operations. Each tool is decorated with `@tool` which makes it usable by the LLM agent.\n",
    "\n",
    "The agent will automatically select and use these tools based on the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b559b6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Defined 7 calculation tools:\n",
      "  - add: Add two numbers together. Use this for addition operations.\n",
      "  - subtract: Subtract b from a. Use this for subtraction operations.\n",
      "  - multiply: Multiply two numbers together. Use this for multiplication operations.\n",
      "  - divide: Divide a by b. Use this for division operations.\n",
      "  - power: Raise base to the power of exponent. Use this for exponentiation.\n",
      "  - square_root: Calculate the square root of x. Use this for square root operations.\n",
      "  - percentage: Calculate percent% of value. For example, 20% of 100 = 20.\n"
     ]
    }
   ],
   "source": [
    "# Define mathematical operation tools\n",
    "# Each function is decorated with @tool to make it available to the agent\n",
    "\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together. Use this for addition operations.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract b from a. Use this for subtraction operations.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together. Use this for multiplication operations.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide a by b. Use this for division operations.\"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "    return a / b\n",
    "\n",
    "\n",
    "@tool\n",
    "def power(base: float, exponent: float) -> float:\n",
    "    \"\"\"Raise base to the power of exponent. Use this for exponentiation.\"\"\"\n",
    "    return base ** exponent\n",
    "\n",
    "\n",
    "@tool\n",
    "def square_root(x: float) -> float:\n",
    "    \"\"\"Calculate the square root of x. Use this for square root operations.\"\"\"\n",
    "    if x < 0:\n",
    "        raise ValueError(\"Cannot calculate square root of negative number\")\n",
    "    return math.sqrt(x)\n",
    "\n",
    "\n",
    "@tool\n",
    "def percentage(value: float, percent: float) -> float:\n",
    "    \"\"\"Calculate percent% of value. For example, 20% of 100 = 20.\"\"\"\n",
    "    return (percent / 100) * value\n",
    "\n",
    "\n",
    "# Create a list of all available tools\n",
    "tools = [add, subtract, multiply, divide, power, square_root, percentage]\n",
    "\n",
    "print(f\"‚úì Defined {len(tools)} calculation tools:\")\n",
    "for tool_func in tools:\n",
    "    print(f\"  - {tool_func.name}: {tool_func.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fff15",
   "metadata": {},
   "source": [
    "### üîç Discovering the Tool Call Structure\n",
    "\n",
    "Let's inspect what GPT-4 returns when it wants to call a tool. This shows you how to discover the keys (`name`, `args`, `id`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722530c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_message = [HumanMessage(content=\"What is 5 plus 3?\")]\n",
    "# response = llm_with_tools.invoke(test_message)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ed60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc9c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's see what GPT-4 returns when it wants to call a tool\n",
    "# # We'll send a simple question and inspect the response\n",
    "\n",
    "# print(\"=\" * 70)\n",
    "# print(\"RESPONSE TYPE:\", type(response))\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# # Check if it has tool_calls\n",
    "# if hasattr(response, 'tool_calls'):\n",
    "#     print(\"\\n‚úÖ Response has 'tool_calls' attribute\")\n",
    "#     print(f\"Number of tool calls: {len(response.tool_calls)}\")\n",
    "    \n",
    "#     # Inspect the first tool call\n",
    "#     if response.tool_calls:\n",
    "#         tool_call = response.tool_calls[0]\n",
    "#         print(f\"\\nüì¶ Tool Call Type: {type(tool_call)}\")\n",
    "#         print(f\"\\nüîë Available Keys: {tool_call.keys() if isinstance(tool_call, dict) else 'Not a dict'}\")\n",
    "#         print(\"\\n\" + \"=\" * 70)\n",
    "#         print(\"FULL TOOL CALL STRUCTURE:\")\n",
    "#         print(\"=\" * 70)\n",
    "        \n",
    "#         # Display the structure\n",
    "#         import json\n",
    "#         print(json.dumps(tool_call, indent=2))\n",
    "        \n",
    "#         print(\"\\n\" + \"=\" * 70)\n",
    "#         print(\"ACCESSING INDIVIDUAL KEYS:\")\n",
    "#         print(\"=\" * 70)\n",
    "#         print(f\"tool_call['name']:  {tool_call['name']}\")\n",
    "#         print(f\"tool_call['args']:  {tool_call['args']}\")\n",
    "#         print(f\"tool_call['id']:    {tool_call['id']}\")\n",
    "#         print(f\"tool_call['type']:  {tool_call.get('type', 'N/A')}\")\n",
    "# else:\n",
    "#     print(\"\\n‚ùå No tool_calls in response\")\n",
    "#     print(f\"Response content: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0dfd9",
   "metadata": {},
   "source": [
    "### üìö Where These Keys Come From\n",
    "\n",
    "**The answer: LangChain standardizes the format!**\n",
    "\n",
    "1. **OpenAI's Raw Response**:\n",
    "   ```json\n",
    "   {\n",
    "     \"tool_calls\": [\n",
    "       {\n",
    "         \"id\": \"call_abc123\",\n",
    "         \"type\": \"function\",\n",
    "         \"function\": {\n",
    "           \"name\": \"add\",\n",
    "           \"arguments\": \"{\\\"a\\\":5,\\\"b\\\":3}\"\n",
    "         }\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **LangChain Converts to Standard Format**:\n",
    "   ```python\n",
    "   {\n",
    "     \"name\": \"add\",              # From function.name\n",
    "     \"args\": {\"a\": 5, \"b\": 3},   # Parsed from function.arguments\n",
    "     \"id\": \"call_abc123\",        # From id\n",
    "     \"type\": \"tool_call\"         # Added by LangChain\n",
    "   }\n",
    "   ```\n",
    "\n",
    "3. **Why This Matters**:\n",
    "   - Different LLM providers (OpenAI, Anthropic, etc.) have different formats\n",
    "   - LangChain **standardizes** them all to use `name`, `args`, `id`\n",
    "   - Your code works with **any LLM** that supports function calling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32a0a1",
   "metadata": {},
   "source": [
    "## Step 2: Define Agent State\n",
    "\n",
    "The agent state keeps track of:\n",
    "- **messages**: The conversation history between user, agent, and tools\n",
    "- **calculation_history**: A log of all calculations performed\n",
    "\n",
    "This state is passed between nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3182f9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent state structure defined\n"
     ]
    }
   ],
   "source": [
    "# Define the state structure for our agent\n",
    "# TypedDict ensures type safety and clarity\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State of the calculator agent that persists across the graph.\"\"\"\n",
    "    \n",
    "    # Messages list stores the conversation history\n",
    "    # The Annotated type with operator.add means new messages are appended to the list\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Track all calculations performed during execution\n",
    "    calculation_history: list[str]\n",
    "\n",
    "\n",
    "print(\"‚úì Agent state structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1004c2",
   "metadata": {},
   "source": [
    "## Step 3: Initialize the Language Model\n",
    "\n",
    "We'll use OpenAI's GPT-4 model and bind our calculation tools to it. This allows the model to:\n",
    "1. Understand the user's question\n",
    "2. Decide which tools to use\n",
    "3. Generate appropriate arguments for the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057fe5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Language model initialized with tools\n",
      "  Model: GPT-4\n",
      "  Available tools: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model with tools\n",
    "# There are multiple ways to set your OpenAI API key:\n",
    "\n",
    "# Method 1: Set environment variable (recommended for production)\n",
    "# Set OPENAI_API_KEY in your system environment variables\n",
    "\n",
    "# Method 2: Set it in code directly (not recommended for production)\n",
    "# Uncomment the line below and replace 'your-api-key-here' with your actual key\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Method 3: Use python-dotenv to load from .env file (recommended for development)\n",
    "# Uncomment the lines below and create a .env file with: OPENAI_API_KEY=your-api-key-here\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Method 4: Pass api_key directly to ChatOpenAI (not recommended)\n",
    "# llm = ChatOpenAI(model=\"gpt-4\", temperature=0, api_key=\"your-api-key-here\")\n",
    "\n",
    "# Create the base LLM (temperature=0 for deterministic responses)\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# Bind the calculation tools to the LLM\n",
    "# This enables the model to call these tools when needed\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"‚úì Language model initialized with tools\")\n",
    "print(\"  Model: GPT-4\")\n",
    "print(f\"  Available tools: {len(tools)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e8901",
   "metadata": {},
   "source": [
    "## Step 4: Define Graph Nodes\n",
    "\n",
    "Our graph has two main nodes:\n",
    "1. **Agent Node**: Calls the LLM to decide what to do\n",
    "2. **Tools Node**: Executes the tools that the agent selected\n",
    "\n",
    "We also need a conditional function to determine if we should continue or end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b84e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Graph nodes and conditional logic defined\n"
     ]
    }
   ],
   "source": [
    "# Node 1: Call the LLM to decide what action to take\n",
    "def call_model(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    This node calls the LLM with the current conversation history.\n",
    "    The LLM will either:\n",
    "    - Call one or more tools (for calculations)\n",
    "    - Respond with a final answer (no more tools needed)\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    print(f\"\"\"Step1: Run the call_model function with message as {state[\"messages\"]}\"\"\")\n",
    "    \n",
    "    # Invoke the LLM with tools\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return the response as a new message to be added to state\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Node 2: Execute the tools that the LLM decided to use\n",
    "def call_tools(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    This node executes all tool calls from the last agent message.\n",
    "    It runs the actual calculations and returns the results.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Get the tool calls from the last AI message\n",
    "    tool_calls = last_message.tool_calls\n",
    "    print(f\"Step3: Run the call_tools function with tools: {tool_calls}\")\n",
    "    # Create a mapping of tool names to tool functions for easy lookup\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    # Execute each tool call and collect results\n",
    "    tool_messages = []\n",
    "    calculation_history = state.get(\"calculation_history\", [])\n",
    "    \n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        \n",
    "        # Get the corresponding tool function\n",
    "        selected_tool = tool_map[tool_name]\n",
    "        \n",
    "        try:\n",
    "            # Execute the tool with the provided arguments\n",
    "            result = selected_tool.invoke(tool_args)\n",
    "            \n",
    "            # Log the calculation for tracking\n",
    "            calc_record = f\"{tool_name}({', '.join(map(str, tool_args.values()))}) = {result}\"\n",
    "            calculation_history.append(calc_record)\n",
    "            \n",
    "            # Create a tool message with the result\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=str(result),\n",
    "                    tool_call_id=tool_id,\n",
    "                    name=tool_name\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Handle any errors during tool execution\n",
    "            tool_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=f\"Error: {str(e)}\",\n",
    "                    tool_call_id=tool_id,\n",
    "                    name=tool_name\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Return updated messages and calculation history\n",
    "    return {\n",
    "        \"messages\": tool_messages,\n",
    "        \"calculation_history\": calculation_history\n",
    "    }\n",
    "\n",
    "\n",
    "# Conditional function: Determine if we should continue or end\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    This function checks if the agent wants to use more tools.\n",
    "    If there are tool calls in the last message, we continue to the tools node.\n",
    "    Otherwise, we end the execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if there are any tool calls in the last message\n",
    "    if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls:\n",
    "        print(\"Step2: Run the should_continue function -- END\")\n",
    "        return \"end\"  # No more tools to call, we're done\n",
    "    \n",
    "    print(\"Step2: Run the should_continue function -- CONTINUE\")\n",
    "    return \"continue\"  # More tools to execute\n",
    "\n",
    "\n",
    "print(\"‚úì Graph nodes and conditional logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41559d05",
   "metadata": {},
   "source": [
    "## Step 5: Build the LangGraph\n",
    "\n",
    "Now we'll construct the graph by:\n",
    "1. Creating a StateGraph with our AgentState\n",
    "2. Adding the agent and tools nodes\n",
    "3. Setting up edges to control the flow\n",
    "4. Compiling the graph\n",
    "\n",
    "The flow will be:\n",
    "```\n",
    "START ‚Üí agent ‚Üí (should_continue?) ‚Üí tools ‚Üí agent ‚Üí ... ‚Üí END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77da22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Calculator graph created and compiled!\n",
      "\n",
      "Graph structure:\n",
      "  START ‚Üí agent ‚Üí [conditional] ‚Üí tools ‚Üí agent ‚Üí ... ‚Üí END\n"
     ]
    }
   ],
   "source": [
    "# Function to create and compile the calculator graph\n",
    "def create_calculator_graph():\n",
    "    \"\"\"\n",
    "    Build the LangGraph calculator agent.\n",
    "    Returns a compiled graph ready for execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the StateGraph with our state schema\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    # \"agent\" node: calls the LLM to make decisions\n",
    "    workflow.add_node(\"agent\", call_model)\n",
    "    \n",
    "    # \"tools\" node: executes the calculation tools\n",
    "    workflow.add_node(\"tools\", call_tools)\n",
    "    \n",
    "    # Set the entry point (where execution starts)\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    \n",
    "    # Add conditional edges from agent node\n",
    "    # Based on should_continue(), either go to tools or END\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",  # From the agent node\n",
    "        should_continue,  # Use this function to decide\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If continue, go to tools node\n",
    "            \"end\": END  # If end, finish execution\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add edge from tools back to agent\n",
    "    # This creates the loop: agent ‚Üí tools ‚Üí agent ‚Üí ...\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    \n",
    "    # Compile the graph into an executable form\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# Create the calculator graph\n",
    "calculator_graph = create_calculator_graph()\n",
    "\n",
    "print(\"‚úì Calculator graph created and compiled!\")\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"  START ‚Üí agent ‚Üí [conditional] ‚Üí tools ‚Üí agent ‚Üí ... ‚Üí END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d79b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000011F2D6A7340>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculator_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98124b80",
   "metadata": {},
   "source": [
    "## Step 6: Create a Helper Function to Run the Agent\n",
    "\n",
    "This function will:\n",
    "- Take a user question as input\n",
    "- Initialize the agent state\n",
    "- Run the graph\n",
    "- Display the calculation steps and final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98124b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Helper function ready to use\n"
     ]
    }
   ],
   "source": [
    "# Helper function to run the calculator agent with a question\n",
    "def run_calculator_agent(question: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Run the calculator agent with a user question.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's calculation question in natural language\n",
    "        verbose: If True, print detailed output\n",
    "    \n",
    "    Returns:\n",
    "        The final state containing all messages and calculation history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the state with the user's question\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=question)],\n",
    "        \"calculation_history\": []\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Invoke the graph with the initial state\n",
    "    # The graph will run until it reaches END\n",
    "    result = calculator_graph.invoke(initial_state)\n",
    "    \n",
    "    if verbose:\n",
    "        # Display all calculation steps\n",
    "        if result.get(\"calculation_history\"):\n",
    "            print(\"Calculation Steps:\")\n",
    "            for i, calc in enumerate(result[\"calculation_history\"], 1):\n",
    "                print(f\"   {i}. {calc}\")\n",
    "            print()\n",
    "        \n",
    "        # Display the final answer from the agent\n",
    "        final_message = result[\"messages\"][-1]\n",
    "        print(f\"‚úÖ Final Answer: {final_message.content}\\n\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úì Helper function ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155e7eb",
   "metadata": {},
   "source": [
    "## Step 7: Test the Calculator Agent\n",
    "\n",
    "Let's test our agent with various calculation questions to see how it handles:\n",
    "- Simple calculations\n",
    "- Multi-step operations\n",
    "- Complex natural language queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb81af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS\n",
      "======================================================================\n",
      "\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS', additional_kwargs={}, response_metadata={})]\n",
      "Step2: Run the should_continue function -- CONTINUE\n",
      "Step3: Run the call_tools function with tools: [{'name': 'add', 'args': {'a': 4, 'b': 8}, 'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'type': 'tool_call'}]\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'function': {'arguments': '{\"a\":4,\"b\":8}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 265, 'total_tokens': 282, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--edd64d22-46f9-45e3-8d75-adca740cd0cc-0', tool_calls=[{'name': 'add', 'args': {'a': 4, 'b': 8}, 'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 265, 'output_tokens': 17, 'total_tokens': 282, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='12.0', name='add', tool_call_id='call_1axvrDGE5qd8KRRGlKT5tvPk')]\n",
      "Step2: Run the should_continue function -- CONTINUE\n",
      "Step3: Run the call_tools function with tools: [{'name': 'divide', 'args': {'a': 12, 'b': 2}, 'id': 'call_w8TvFH1NvRYr4mqQhv1PcWBO', 'type': 'tool_call'}]\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'function': {'arguments': '{\"a\":4,\"b\":8}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 265, 'total_tokens': 282, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--edd64d22-46f9-45e3-8d75-adca740cd0cc-0', tool_calls=[{'name': 'add', 'args': {'a': 4, 'b': 8}, 'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 265, 'output_tokens': 17, 'total_tokens': 282, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='12.0', name='add', tool_call_id='call_1axvrDGE5qd8KRRGlKT5tvPk'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_w8TvFH1NvRYr4mqQhv1PcWBO', 'function': {'arguments': '{\"a\":12,\"b\":2}', 'name': 'divide'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 292, 'total_tokens': 309, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c3b7acc5-9c88-440f-bece-5523306f5625-0', tool_calls=[{'name': 'divide', 'args': {'a': 12, 'b': 2}, 'id': 'call_w8TvFH1NvRYr4mqQhv1PcWBO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 17, 'total_tokens': 309, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='6.0', name='divide', tool_call_id='call_w8TvFH1NvRYr4mqQhv1PcWBO')]\n",
      "Step2: Run the should_continue function -- CONTINUE\n",
      "Step3: Run the call_tools function with tools: [{'name': 'power', 'args': {'base': 2, 'exponent': 6}, 'id': 'call_YnN5bTO08V0u2zTw4Ut30NaE', 'type': 'tool_call'}]\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'function': {'arguments': '{\"a\":4,\"b\":8}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 265, 'total_tokens': 282, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--edd64d22-46f9-45e3-8d75-adca740cd0cc-0', tool_calls=[{'name': 'add', 'args': {'a': 4, 'b': 8}, 'id': 'call_1axvrDGE5qd8KRRGlKT5tvPk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 265, 'output_tokens': 17, 'total_tokens': 282, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='12.0', name='add', tool_call_id='call_1axvrDGE5qd8KRRGlKT5tvPk'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_w8TvFH1NvRYr4mqQhv1PcWBO', 'function': {'arguments': '{\"a\":12,\"b\":2}', 'name': 'divide'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 292, 'total_tokens': 309, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c3b7acc5-9c88-440f-bece-5523306f5625-0', tool_calls=[{'name': 'divide', 'args': {'a': 12, 'b': 2}, 'id': 'call_w8TvFH1NvRYr4mqQhv1PcWBO', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 17, 'total_tokens': 309, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='6.0', name='divide', tool_call_id='call_w8TvFH1NvRYr4mqQhv1PcWBO'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YnN5bTO08V0u2zTw4Ut30NaE', 'function': {'arguments': '{\"base\":2,\"exponent\":6}', 'name': 'power'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 319, 'total_tokens': 337, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--127b61d3-32c1-4b49-bfd4-b79445e5f7c1-0', tool_calls=[{'name': 'power', 'args': {'base': 2, 'exponent': 6}, 'id': 'call_YnN5bTO08V0u2zTw4Ut30NaE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 319, 'output_tokens': 18, 'total_tokens': 337, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='64.0', name='power', tool_call_id='call_YnN5bTO08V0u2zTw4Ut30NaE')]\n",
      "Step2: Run the should_continue function -- END\n",
      "Calculation Steps:\n",
      "   1. add(4, 8) = 12.0\n",
      "   2. divide(12, 2) = 6.0\n",
      "   3. power(2, 6) = 64.0\n",
      "\n",
      "‚úÖ Final Answer: The result of the calculation is 64. I LIKE HUMANS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple addition\n",
    "myresult = run_calculator_agent(\"add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With each iteration through the graph, the message list grows\n",
    "# # Iteration 1:\n",
    "# #    Messages: 1\n",
    "# #    [HumanMessage(\"add 4 and 8 then divide by 2\")]\n",
    "# #    ‚Üí GPT-4 decides to call add(4, 8)\n",
    "# #   After tools execute:\n",
    "# #   Messages: 3\n",
    "# #   [HumanMessage(...),\n",
    "# #   AIMessage(tool_calls=[add(4,8)]),\n",
    "# #   ToolMessage(\"12.0\")]   \n",
    "\n",
    "\n",
    "# # Iteration 2:\n",
    "# #     Messages: 3 (same as above, passed to GPT-4 again)\n",
    "# #     ‚Üí GPT-4 sees the result 12.0, decides to call divide(12, 2)\n",
    "# #     After tools execute:\n",
    "# #     Messages: 5\n",
    "# #     [HumanMessage(...),\n",
    "# #     AIMessage(tool_calls=[add(4,8)]),\n",
    "# #     ToolMessage(\"12.0\"),\n",
    "# #     AIMessage(tool_calls=[divide(12,2)]),\n",
    "# #     ToolMessage(\"6.0\")]\n",
    "    \n",
    "# #     and so on\n",
    "\n",
    "# # Why This Happens:\n",
    "# #     The Annotated[Sequence[BaseMessage], operator.add] annotation means:\n",
    "\n",
    "# # New messages are APPENDED to the list\n",
    "# # Nothing is removed\n",
    "# # The list grows by 2 messages per iteration (1 AIMessage + 1 ToolMessage)\n",
    "# # Eventually ends with a final AIMessage (text only, no tools)\n",
    "\n",
    "\n",
    "# Pros:\n",
    "# ‚úÖ Full context retention - GPT-4 sees entire conversation history\n",
    "# ‚úÖ Better decision making - Model can reason about all previous steps\n",
    "# ‚úÖ Standard LangChain pattern - Well-documented and supported\n",
    "# ‚úÖ Debugging friendly - You can trace the entire execution\n",
    "# ‚úÖ Stateful conversations - Natural for multi-turn interactions\n",
    "# Cons:\n",
    "# ‚ö†Ô∏è Token costs increase - More messages = more tokens per API call\n",
    "# ‚ö†Ô∏è Context window limits - Very long conversations might hit limits (but rare for calculations)\n",
    "# ‚ö†Ô∏è Slower over time - Processing more messages takes slightly longer\n",
    "\n",
    "# Alternative Approaches\n",
    "# 1. Message Summarization (For Very Long Conversations)\n",
    "# # Periodically summarize old messages\n",
    "# if len(messages) > 10:\n",
    "#     summary = \"Previous calculations: add(4,8)=12, divide(12,2)=6...\"\n",
    "#     messages = [HumanMessage(summary), ...recent_messages]\n",
    "# Use when: Conversations get extremely long (50+ messages)\n",
    "\n",
    "# 2. Only Keep Recent Messages\n",
    "# # Keep only last N messages\n",
    "# messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "# def call_model(state):\n",
    "#     all_messages = state[\"messages\"]\n",
    "#     recent_messages = all_messages[-5:]  # Last 5 only\n",
    "#     response = llm_with_tools.invoke(recent_messages)\n",
    "# Use when: You only care about recent context, not full history\n",
    "\n",
    "\n",
    "# 3. Separate Context from Messages\n",
    "# class AgentState(TypedDict):\n",
    "#     messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "#     context: str  # Store summary separately\n",
    "    \n",
    "# def call_model(state):\n",
    "#     # Pass context as system message\n",
    "#     system = SystemMessage(f\"Context: {state['context']}\")\n",
    "#     response = llm.invoke([system] + state[\"messages\"][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "895341c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='add 4 and 8 then divide it by 2. Then do 2^ of this result. In the end give me result and say I LIKE HUMANS', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_8MZc1HXkEsiU2hnHxlwRDoeT', 'function': {'arguments': '{\"a\":4,\"b\":8}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 265, 'total_tokens': 282, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f9d5fc88-37bf-43a8-b21c-96a7c65590f0-0', tool_calls=[{'name': 'add', 'args': {'a': 4, 'b': 8}, 'id': 'call_8MZc1HXkEsiU2hnHxlwRDoeT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 265, 'output_tokens': 17, 'total_tokens': 282, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='12.0', name='add', tool_call_id='call_8MZc1HXkEsiU2hnHxlwRDoeT'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_DaQdp03Ej2YfJCiosAd50vE2', 'function': {'arguments': '{\"a\":12,\"b\":2}', 'name': 'divide'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 292, 'total_tokens': 309, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c1c905f1-4527-42a3-a355-bdfec544120d-0', tool_calls=[{'name': 'divide', 'args': {'a': 12, 'b': 2}, 'id': 'call_DaQdp03Ej2YfJCiosAd50vE2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 292, 'output_tokens': 17, 'total_tokens': 309, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='6.0', name='divide', tool_call_id='call_DaQdp03Ej2YfJCiosAd50vE2'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qmUl0g2VebZpSXoacqefmIkt', 'function': {'arguments': '{\"base\":2,\"exponent\":6}', 'name': 'power'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 319, 'total_tokens': 337, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d22318ff-e5bd-4175-925d-8b4d46fa67d3-0', tool_calls=[{'name': 'power', 'args': {'base': 2, 'exponent': 6}, 'id': 'call_qmUl0g2VebZpSXoacqefmIkt', 'type': 'tool_call'}], usage_metadata={'input_tokens': 319, 'output_tokens': 18, 'total_tokens': 337, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='64.0', name='power', tool_call_id='call_qmUl0g2VebZpSXoacqefmIkt'),\n",
       "  AIMessage(content='The result of the calculation is 64. I LIKE HUMANS.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 347, 'total_tokens': 362, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'stop', 'logprobs': None}, id='run--4e706a76-e249-4994-83cb-1765dd8baa5f-0', usage_metadata={'input_tokens': 347, 'output_tokens': 15, 'total_tokens': 362, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'calculation_history': ['add(4, 8) = 12.0',\n",
       "  'divide(12, 2) = 6.0',\n",
       "  'power(2, 6) = 64.0']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66643f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: What is (50 + 30) multiplied by 2, then find 25% of that result?\n",
      "======================================================================\n",
      "\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='What is (50 + 30) multiplied by 2, then find 25% of that result?', additional_kwargs={}, response_metadata={})]\n",
      "Step2: Run the should_continue function -- CONTINUE\n",
      "Step3: Run the call_tools function with tools: [{'name': 'add', 'args': {'a': 50, 'b': 30}, 'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'type': 'tool_call'}]\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='What is (50 + 30) multiplied by 2, then find 25% of that result?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'function': {'arguments': '{\"a\":50,\"b\":30}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 253, 'total_tokens': 270, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--be135503-73a8-442a-83e0-7fe959e96de0-0', tool_calls=[{'name': 'add', 'args': {'a': 50, 'b': 30}, 'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 17, 'total_tokens': 270, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='80.0', name='add', tool_call_id='call_xnsNhNhxMiAvPn5ZCpx5NMci')]\n",
      "Step2: Run the should_continue function -- CONTINUE\n",
      "Step3: Run the call_tools function with tools: [{'name': 'multiply', 'args': {'a': 80, 'b': 2}, 'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'type': 'tool_call'}]\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='What is (50 + 30) multiplied by 2, then find 25% of that result?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'function': {'arguments': '{\"a\":50,\"b\":30}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 253, 'total_tokens': 270, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--be135503-73a8-442a-83e0-7fe959e96de0-0', tool_calls=[{'name': 'add', 'args': {'a': 50, 'b': 30}, 'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 17, 'total_tokens': 270, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='80.0', name='add', tool_call_id='call_xnsNhNhxMiAvPn5ZCpx5NMci'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'function': {'arguments': '{\"a\":80,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 280, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bf73fb9c-65ae-43ef-8c5e-416c3af81acc-0', tool_calls=[{'name': 'multiply', 'args': {'a': 80, 'b': 2}, 'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 280, 'output_tokens': 17, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='160.0', name='multiply', tool_call_id='call_caDW7BDq1318rwZ9w8xXRYAp')]\n",
      "Step2: Run the should_continue function -- CONTINUE\n",
      "Step3: Run the call_tools function with tools: [{'name': 'percentage', 'args': {'value': 160, 'percent': 25}, 'id': 'call_sjmx6yicXbM601ujSKaUdo7D', 'type': 'tool_call'}]\n",
      "Step1: Run the call_model function with message as [HumanMessage(content='What is (50 + 30) multiplied by 2, then find 25% of that result?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'function': {'arguments': '{\"a\":50,\"b\":30}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 253, 'total_tokens': 270, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--be135503-73a8-442a-83e0-7fe959e96de0-0', tool_calls=[{'name': 'add', 'args': {'a': 50, 'b': 30}, 'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 17, 'total_tokens': 270, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='80.0', name='add', tool_call_id='call_xnsNhNhxMiAvPn5ZCpx5NMci'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'function': {'arguments': '{\"a\":80,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 280, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bf73fb9c-65ae-43ef-8c5e-416c3af81acc-0', tool_calls=[{'name': 'multiply', 'args': {'a': 80, 'b': 2}, 'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 280, 'output_tokens': 17, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='160.0', name='multiply', tool_call_id='call_caDW7BDq1318rwZ9w8xXRYAp'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sjmx6yicXbM601ujSKaUdo7D', 'function': {'arguments': '{\"value\":160,\"percent\":25}', 'name': 'percentage'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 307, 'total_tokens': 325, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--a56a661c-b668-42ad-979f-d6a2740a16fb-0', tool_calls=[{'name': 'percentage', 'args': {'value': 160, 'percent': 25}, 'id': 'call_sjmx6yicXbM601ujSKaUdo7D', 'type': 'tool_call'}], usage_metadata={'input_tokens': 307, 'output_tokens': 18, 'total_tokens': 325, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='40.0', name='percentage', tool_call_id='call_sjmx6yicXbM601ujSKaUdo7D')]\n",
      "Step2: Run the should_continue function -- END\n",
      "Calculation Steps:\n",
      "   1. add(50, 30) = 80.0\n",
      "   2. multiply(80, 2) = 160.0\n",
      "   3. percentage(160, 25) = 40.0\n",
      "\n",
      "‚úÖ Final Answer: (50 + 30) multiplied by 2 is 160, and 25% of that result is 40.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is (50 + 30) multiplied by 2, then find 25% of that result?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'function': {'arguments': '{\"a\":50,\"b\":30}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 253, 'total_tokens': 270, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--be135503-73a8-442a-83e0-7fe959e96de0-0', tool_calls=[{'name': 'add', 'args': {'a': 50, 'b': 30}, 'id': 'call_xnsNhNhxMiAvPn5ZCpx5NMci', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 17, 'total_tokens': 270, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='80.0', name='add', tool_call_id='call_xnsNhNhxMiAvPn5ZCpx5NMci'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'function': {'arguments': '{\"a\":80,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 280, 'total_tokens': 297, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bf73fb9c-65ae-43ef-8c5e-416c3af81acc-0', tool_calls=[{'name': 'multiply', 'args': {'a': 80, 'b': 2}, 'id': 'call_caDW7BDq1318rwZ9w8xXRYAp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 280, 'output_tokens': 17, 'total_tokens': 297, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='160.0', name='multiply', tool_call_id='call_caDW7BDq1318rwZ9w8xXRYAp'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sjmx6yicXbM601ujSKaUdo7D', 'function': {'arguments': '{\"value\":160,\"percent\":25}', 'name': 'percentage'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 307, 'total_tokens': 325, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--a56a661c-b668-42ad-979f-d6a2740a16fb-0', tool_calls=[{'name': 'percentage', 'args': {'value': 160, 'percent': 25}, 'id': 'call_sjmx6yicXbM601ujSKaUdo7D', 'type': 'tool_call'}], usage_metadata={'input_tokens': 307, 'output_tokens': 18, 'total_tokens': 325, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='40.0', name='percentage', tool_call_id='call_sjmx6yicXbM601ujSKaUdo7D'),\n",
       "  AIMessage(content='(50 + 30) multiplied by 2 is 160, and 25% of that result is 40.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 335, 'total_tokens': 361, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'finish_reason': 'stop', 'logprobs': None}, id='run--e31ec05a-ae99-4eef-86d4-084996648e30-0', usage_metadata={'input_tokens': 335, 'output_tokens': 26, 'total_tokens': 361, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'calculation_history': ['add(50, 30) = 80.0',\n",
       "  'multiply(80, 2) = 160.0',\n",
       "  'percentage(160, 25) = 40.0']}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_question = \"What is (50 + 30) multiplied by 2, then find 25% of that result?\"\n",
    "\n",
    "run_calculator_agent(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7acce1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Built:\n",
    "‚úÖ A LangGraph agent that can perform multiple calculations based on natural language questions\n",
    "\n",
    "‚úÖ 7 different calculation tools (add, subtract, multiply, divide, power, square_root, percentage)\n",
    "\n",
    "‚úÖ A state management system to track conversation and calculation history\n",
    "\n",
    "‚úÖ A graph-based workflow that loops between decision-making (agent) and action-taking (tools)\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Tools**: Functions decorated with `@tool` that the LLM can call\n",
    "2. **State**: Persistent data structure that flows through the graph\n",
    "3. **Nodes**: Functions that process the state (call_model, call_tools)\n",
    "4. **Edges**: Connections between nodes that control the flow\n",
    "5. **Conditional Edges**: Decision points in the graph based on state\n",
    "\n",
    "### How It Works:\n",
    "1. User asks a question in natural language\n",
    "2. Agent (LLM) analyzes the question and decides which tools to use\n",
    "3. Tools are executed with appropriate arguments\n",
    "4. Results are fed back to the agent\n",
    "5. Agent decides if more calculations are needed or if it can answer\n",
    "6. Process repeats until final answer is ready\n",
    "\n",
    "### Next Steps:\n",
    "- Add more mathematical tools (trigonometry, logarithms, etc.)\n",
    "- Implement memory to reference previous calculations\n",
    "- Add visualization of calculation steps\n",
    "- Integrate with external APIs for advanced calculations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
